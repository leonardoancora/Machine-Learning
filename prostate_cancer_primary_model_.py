# -*- coding: utf-8 -*-
"""Prostate_Cancer_Primary_Model_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/129PQ8R0520PJIBHAQR7vndxnaszgAkvr

# Nuova sezione

# *Libraries*
"""

!pip install catboost

!pip install shap

from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier,StackingClassifier,VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold,RepeatedStratifiedKFold
from keras.regularizers import l2
from numpy import savetxt
from sklearn.neighbors import LocalOutlierFactor
from sklearn.feature_selection import chi2
from sklearn.preprocessing import QuantileTransformer
from keras.constraints import unit_norm
from keras.callbacks import EarlyStopping
from imblearn.over_sampling import SMOTE
from keras.models import Sequential
from torch.utils.data import Dataset, DataLoader
from sklearn.ensemble import AdaBoostRegressor
from keras.wrappers.scikit_learn import KerasRegressor,KerasClassifier
from keras.layers import Dense,TimeDistributed
from tensorflow.python.framework import ops
from keras.layers import BatchNormalization
import torch.utils.data as data_utils
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import plot_roc_curve
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
import tensorflow as tf
import pandas as pd
import torch
import tensorflow as tf
import torch.nn as nn
import torch.nn.functional as F
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
from keras import regularizers
from sklearn.externals.joblib import dump,load
from sklearn.externals import joblib
import sklearn
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler,RobustScaler
from sklearn.metrics import mean_absolute_error,mean_squared_error
import os
from sklearn.metrics import confusion_matrix
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn import preprocessing
import numpy as np
from sklearn.metrics import accuracy_score, plot_confusion_matrix
import matplotlib.pyplot as plt
from keras.preprocessing import sequence 
from keras.models import Sequential
from sklearn.metrics import classification_report 
from keras.layers import Dense,Dropout
from numpy import array
from tensorflow import keras
from pandas import DataFrame
from matplotlib import pyplot
from math import sqrt
from sklearn.metrics import mean_squared_error
from numpy import concatenate
from sklearn.metrics import f1_score
import pickle as pkl
from sklearn.decomposition import PCA
import seaborn as sns
from sklearn.metrics import f1_score
from sklearn.svm import SVC,LinearSVC,LinearSVR
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif,VarianceThreshold,mutual_info_classif,RFE,RFECV
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate
import pickle
from catboost import CatBoostClassifier, Pool, cv
import catboost
from catboost import *
import shap

#main_data_red=pd.read_csv('/content/STUDY 1_2_3 FULL DATASET - Primary Model.csv')

#main_data_red=pd.read_csv('/content/STUDY 123 DATASETS no_nansubs bilinorm imputs no_derivs reduced - final 25 feb 2021.csv')

main_data_red=pd.read_csv('/content/STUDY 123 DATASETS no_nansubs bilinorm imputs no_derivs leo reduced no_bili - 10 mar leo add psab.csv')

#main_data_red=pd.read_csv('/content/Dataset_backup_shuffle.csv')

#main_data_red=pd.read_csv('/content/Study 1,2,3_Dataset_tofeed_named.csv',header=None)

#main_data_red.drop(columns=['sub_num'],inplace=True)
main_data_red

print(main_data_red.info())

main_data_red=main_data_red.astype('float32') #let's convert all the values into float

main_data_red

print(main_data_red.info())

#inspecting if there is any NAN value 
main_data_red.isnull().sum()

main_data_red.isnull().values.any()

"""We plot all the variables that are just concerning demographics,laboratory examinations, operations and clinical stage of the patients."""

main_data_red.loc[:,'TARGET good_responder_psa_3m_2ymean_>0.2psaincrease':'alt_start'].hist(bins=10,figsize=(16,16))

"""As we can see the **good responder** class, which is our target variable is almost perfectly balanced. This was the result to use as a proxy, instead of cancer progression one among the increase over 0.2 in PSA levels ,the presence of serious adverse event, death related to cancer spread and dropout from the treatmentin within the two years of trial.

This for what concerns the other medications already taken grouped by active drug ingredient:
"""

main_data_red.loc[:,'GENTEXT_AMLODIPINE':'GENTEXT_WARFARIN'].hist(bins=10,figsize=(25,25))

"""Finally we can check the last part which contains the drugs category variables:"""

main_data_red.loc[:,'PCLATEXT_ACETIC ACID DERIVATIVES AND RELATED SUBSTANCES':].hist(bins=10,figsize=(35,35))

main_data_red

main_data_red.loc[:,'mid_age_gp':'alt_start']

Race_Feature=pd.get_dummies(main_data_red['cauc_black_asian_hisp_other'])
Race_Feature.columns=['Caucasian','Black','Asian','Hispanic','Other']
Race_Feature=Race_Feature.drop(columns='Other')
Gleason_Feature=pd.get_dummies(main_data_red['gleason_base_cat'])
Gleason_Feature.columns=['Gleason1','Gleason2','Gleason3','Gleason4']
Gleason_Feature.drop(columns='Gleason4',inplace=True)
clinstage_base2=pd.get_dummies(main_data_red['clinstage_base2'])
clinstage_base2.columns=['Clinical1','Clinical2','Clinical3','Clinical4','Clinical5','Clinical6']
clinstage_base2.drop(columns='Clinical6',inplace=True)

Race_Feature.dtypes

a=main_data_red.loc[:,'GENTEXT_AMLODIPINE': ]
b=pd.concat([Race_Feature,Gleason_Feature,clinstage_base2,a],axis=1)

c=main_data_red.loc[:,'TARGET good_responder_psa_3m_2ymean_>0.2psaincrease':'alt_start']
c=c.drop(columns=['cauc_black_asian_hisp_other','gleason_base_cat','clinstage_base2','prev_radio therapy','metast_node_disease','prostatect '])
d=main_data_red[['prostatect ','prev_radio therapy','metast_node_disease']]
main_data_red=pd.concat([c,d,b],axis=1)

main_data_red

"""# Feature Selection

Let's now catch those features that could be redundant, potentially there could be some high correlation among drug category and active drug ingredient.

##Chi Squared
"""

X=main_data_red.iloc[:,1:]
y=main_data_red.iloc[:,:1]

X_cat=main_data_red.iloc[:,7:]
y=main_data_red.iloc[:,:1]
X_cat

chi_scores = chi2(X_cat,y)

p_values=pd.Series(chi_scores[1],index=X_cat.columns)
p_values.sort_values(ascending=False,inplace=True)
plt.figure(figsize=(20,10))
p_values.plot.bar()

chi_values=pd.Series(chi_scores[0],index=X_cat.columns)
chi_values.sort_values(ascending=False,inplace=True)
plt.figure(figsize=(20,10))
chi_values.plot.bar()

index_to_drop=np.where(chi_scores[1]>0.05)
index_to_drop

X_cat.drop(X_cat.columns[index_to_drop],axis=1,inplace=True)

Gleason_Feature=pd.get_dummies(X_cat['gleason_base_cat'])
Gleason_Feature.columns=['Gleason1','Gleason2','Gleason3','Gleason4']
Gleason_Feature.drop(columns='Gleason4',inplace=True)

#X_cat=X_cat.drop(columns=['gleason_base_cat'])
X_cat=pd.concat([X_cat,Gleason_Feature],axis=1)
X_cat

"""##F1 Score"""

X_num=main_data_red.iloc[:,1:7]
X_num

X_num=main_data_red.iloc[:,19:]
X_num

X_num.skew()

X_num=main_data_red.iloc[:,1:7]
X_num



selector = SelectKBest(f_classif,k='all')
selected_features = selector.fit(X_num,y)
plt.figure(figsize=(35,25))
plt.ylabel('F_score',fontsize=30)
plt.xlabel('Features',fontsize=30)
plt.xticks(rotation=90,fontsize=15)
plt.xticks(np.arange(0,30,1))
plt.yticks(np.arange(0,800,5))
sns.lineplot(x=X_num.columns,y=selector.scores_.round(),marker="o")

selector = SelectKBest(f_classif,k='all')
selected_features = selector.fit(X_num,y)

"""According the F-score calculation, the best predictors for our efficacy treament are: **gleason baseline,angina,potassium chloride and supplements.** We decide to remove those which have score equal to 0."""

selected_features_index=np.where(selector.scores_ > 20)[0]

selector.pvalues_

index_to_drop=np.where(selector.pvalues_>0.05)

index_to_drop

selector.scores_

X=pd.concat([X_num,X_cat],axis=1)
X

#columns_X=X.columns
#columns_X_f1=X_f1.columns
#scaler=MinMaxScaler(feature_range=(0,1))
#scaler=StandardScaler()
#scaler=QuantileTransformer()
scaler=RobustScaler()
#X=scaler.fit_transform(X_train)
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)
#X_f1=scaler.fit_transform(X_f1)
#X= preprocessing.StandardScaler().fit(X).transform(X)
#X_f1= preprocessing.StandardScaler().fit(X_f1).transform(X_f1)

#scaler=QuantileTransformer()

X_train, y_train = X_train[mask, :], y_train[mask]

print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

X_f1=X.iloc[:,selected_features_index]

X_f1

#X_f1.drop(columns=['psa_prior_BIGIMPUT'],inplace=True)

X_f1

corr=X.corr()

corr.rename(index={0: "x"},inplace=True)

corr.reset_index()



"""Let's plot those which have a correlation higher than 90%:"""

kot = corr[corr>=.7]
plt.figure(figsize=(20,12))
sns.heatmap(kot, cmap="Greens")

"""Let's sort those which are more than 90% correlated:




"""

s = corr.unstack()
s=s.sort_values(ascending=False)
a=(s>.9)&(s<1)|(s<-.7)
a=s[a]
a=pd.DataFrame(data=a)
a.drop_duplicates(inplace=True)
a

"""Clearly we have high correaltion among the active principles and some drug categories(paracetamol and aspirin with ANALGESICS AND ANTIPYRETICS, ANILIDES)."""

#main_data_red.drop(columns=['GENTEXT_FUROSEMIDE','PCLATEXT_ANALGESICS AND ANTIPYRETICS, SALICYLIC ACID AND DERIVATIVES','PCLATEXT_DIGITALIS GLYCOSIDES','GENTEXT_OMEPRAZOLE'],inplace=True,axis=1)
main_data_red=pd.concat([main_data_red.iloc[:,:7],main_data_red.iloc[:,7:].astype('int64')],axis=1)

corr=main_data_red.corr()

a.columns=(['1,2,3'])

correlated_feat=pd.DataFrame(data=corr.iloc[:,0])
correlated_feat.reset_index(inplace=True)

for i,r in a.iterrows():
  print(correlated_feat[correlated_feat['index']==i[0]])
  print(correlated_feat[correlated_feat['index']==i[1]])

X=X.drop(columns=['PCLATEXT_ANALGESICS AND ANTIPYRETICS, SALICYLIC ACID AND DERIVATIVES'],axis=1)

X=main_data_red.iloc[:,1:]
y=main_data_red.iloc[:,:1]
#X=X.drop(columns=['bili_start_NORM'],axis=1)

X.iloc[:,2:].skew()

X_train_backup, X_test_backup, y_train, y_test = train_test_split( X, y.values, test_size=0.3, shuffle=True,stratify=y)
X_train=X_train_backup.iloc[:,2:]
X_test=X_test_backup.iloc[:,2:]
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

Export=pd.concat([X_train_backup,X_test_backup],axis=0)
exp=np.concatenate([y_train,y_test],axis=0)
y_exp=pd.DataFrame(data=exp)
backup=pd.concat([y_exp,Export,],axis=1,sort=False)
Export.to_csv('/content/backupx.csv',index=False)
y_exp.to_csv('/content/backupy.csv',index=False)

"""# Features Normalization
We can proceed now normalize the feature values in the interval (0,1) with the MinMax method.
"""

X_train, X_test, y_train, y_test = train_test_split( X, y.values, test_size=0.3, shuffle=True, stratify=y)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

#columns_X=X.columns
#columns_X_f1=X_f1.columns
#scaler=MinMaxScaler(feature_range=(0,1))
scaler=StandardScaler()
scaler=QuantileTransformer()
#X=scaler.fit_transform(X_train)
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)
#X_f1=scaler.fit_transform(X_f1)
#X= preprocessing.StandardScaler().fit(X).transform(X)
#X_f1= preprocessing.StandardScaler().fit(X_f1).transform(X_f1)

X_train.iloc[:,15:]



scaler=RobustScaler()
#scaler=StandardScaler()
#scaler=QuantileTransformer()
#scaler=MinMaxScaler()
X_train_norm=scaler.fit_transform(X_train.iloc[:,15:])
X_test_norm=scaler.transform(X_test.iloc[:,15:])
X_train=np.concatenate([X_train_norm,X_train.iloc[:,:15].astype('int64')],axis=1,)
X_test=np.concatenate([X_test_norm,X_test.iloc[:,:15].astype('int64')],axis=1)

print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

"""To take out some of the redundancies we can apply the **Chi Squared** which gave us the best predictors:

# Outliers inspection

Let's now give a look to the outliers present in the distribution, excluding the one hot encoded variables.
"""

columns_plot=X.columns[(X.max() != 1) | X.min()!=0 ]

columns_plot=columns_plot.to_list()

outliers_to_plot=X[columns_plot]

outliers_to_plot=pd.concat([outliers_to_plot,y],axis=1)
outliers_to_plot.rename(columns={'TARGET good_responder_psa_3m_2ymean_>0.2psaincrease':'good_responder'},inplace=True)

outliers_to_plot

outliers_to_plot=outliers_to_plot[['mid_age_gp','gleason_base_cat','weight_cat','clinstage_base2','bili_start','ast_start','alt_start','good_responder']]

outliers_to_plot

fig, axes = plt.subplots(1,7,figsize=(20,8))
fig.tight_layout()
for i,a in zip(outliers_to_plot,axes):
  if i !='good_responder': 
    sns.boxplot(y=i,x=outliers_to_plot['good_responder'],data=outliers_to_plot,palette='Set1',ax=a)

"""**ALT**,**AST** and **BILI** variables are the ones with the highest number of outliers: typically the range for normal AST is reported between 10 to 40 units per liter and ALT between 7 to 56 units per liter. Mild elevations are generally considered to be **2-3 times higher than the normal range**,  therefore we don't need to delete them. """

font = {'family': 'serif',
        'color':  'black',
        'weight': 'normal',
        'size': 12,
        }
fig = plt.figure(figsize=(15,10))
ax = fig.add_subplot(111, projection='3d')
ax.set_title('Good Responders vs Age, Gleason score and Clinical Stage',fontdict={'family': 'serif','color':  'black','weight': 'normal','size': 16})
ax.set_xlabel('Age',fontdict=font)
ax.set_ylabel('Gleason Score',fontdict=font)
ax.set_zlabel('Clinical Stage',fontdict=font)
#ax.set_yticks(np.arange(0,6,0.5))
sp = ax.scatter(outliers_to_plot.iloc[:,0],outliers_to_plot.iloc[:,1],outliers_to_plot.iloc[:,3], s=50, c=outliers_to_plot.iloc[:,7],cmap='RdYlGn')
plt.colorbar(sp)
ax.view_init(50)

"""We can see that all the good responders are more distributed below the **5th** level of clinical stage (T2a): where tumor has invaded one-half (or less) of one side of the prostate.
Gleason score in the interval **(7-10)**  and age **below 60**.

#Dimensionality Reduction
We try to apply Principal Component Analysis to possibly give to the model a lower dimensional space to contain the data.
"""

pca = PCA()
X_f1_pca = pca.fit_transform(X_train)

explained_variance = pca.explained_variance_ratio_
plt.figure(figsize=(11,7))
plt.title('Explained Variance')
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.yticks(np.arange(0,1.1,0.05))
plt.xticks(np.arange(0,125,5))
plt.plot(np.cumsum(explained_variance))
plt.plot(35,0.95,marker='o')

explained_variance=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3))
explained_variance

explained_variance.shape

"""With 3 Components we can explain 95% of Variance:"""

pca_comp = PCA(n_components=34)
#X_f1_pca = pca_comp.fit_transform(X)

X_train=pca_comp.fit_transform(X_train)
X_test=pca_comp.transform(X_test)

Components=pd.DataFrame(data=np.round(explained_variance, decimals=3)*100)
Components['Component Name']=X.columns
Components.columns=['Score','Component Name']

# number of components
n_pcs= pca.components_.shape[0]

# get the index of the most important feature on EACH component
# LIST COMPREHENSION HERE
most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]

initial_feature_names = X.columns
# get the names
most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]

# LIST COMPREHENSION HERE AGAIN
dic = {'Principal Component {}'.format(i): most_important_names[i] for i in range(n_pcs)}

# build the dataframe
components = pd.DataFrame(dic.items())

components['Score']=(pca.explained_variance_ratio_*100).astype('float32').round(decimals=2)

components.columns=['PC','Feature','Score']

components

sns.set_theme(style="whitegrid")

# Initialize the matplotlib figure
f, ax = plt.subplots(figsize=(20, 20))


sns.set_color_codes("pastel")
sns.barplot(x='Score', y='Feature', data=components,
            label="Total", color="b")

# Add a legend and informative axis label
ax.legend( loc="lower right", frameon=True)
ax.set(xlim=(0, 10), ylabel="Feature",
       xlabel="Principal Components Analysis")
sns.despine(left=True, bottom=True)



"""# **Model**

In this section we'll start modelling, with simplest some of the most Ppopular machine learning algorithms for binary classification.
"""

y=y.rename(columns={'TARGET good_responder_psa_3m_2ymean_>0.2psaincrease':'good_responder'})

"""# Support Vector Machine"""

C=np.arange(0.02,0.11,0.01)
gamma=np.arange(1.5,10,0.5)

print(C)
print(gamma)

SVM= SVC()
svm_grid_values = {'C':[0.005,0.007,0.001,0.01], 'gamma':[0.5,1,1.5,2], 'kernel': ['rbf', 'poly', 'sigmoid']}
scoring=['accuracy','precision','recall','roc_auc']
results=[]
for scoring in scoring:
  print(scoring)
  svm_gridsearch = GridSearchCV(SVM, param_grid = svm_grid_values,scoring = scoring)
  svm_best_params = svm_gridsearch.fit(X_train, y_train.ravel())
  best_C = svm_best_params.best_estimator_.get_params()['C']
  best_gamma = svm_best_params.best_estimator_.get_params()['gamma']
  best_kernel = svm_best_params.best_estimator_.get_params()['kernel']
  svm_tuned =SVC(C= best_C, gamma = best_gamma , kernel = best_kernel)
  svm_tuned.fit(X_train,y_train.ravel())
  svm_scores = cross_validate(svm_tuned, X_train, y_train.ravel(), cv=5, scoring=('accuracy','precision','recall','roc_auc'), return_train_score=True)
  svm_mn_acc = np.mean(list(svm_scores.values())[2])
  #svm_mn_f1 = np.mean(list(svm_scores.values())[3])
  svm_mn_precis = np.mean(list(svm_scores.values())[4])
  svm_mn_recall = np.mean(list(svm_scores.values())[5])
  svm_mn_auc = np.mean(list(svm_scores.values())[6])
  global_mean=np.mean([np.array(svm_mn_acc),np.array(svm_mn_precis),np.array(svm_mn_recall),np.array(svm_mn_auc)])
  #results.append([scoring,svm_mn_acc,svm_mn_f1,svm_mn_precis,svm_mn_recall,svm_mn_auc])
  results.append([scoring,global_mean,best_C,best_gamma,best_kernel])
  #print(f'for {scoring} accuracy of:{svm_mn_acc} f1:{svm_mn_f1} precision:{svm_mn_precis} recall:{svm_mn_recall} auc:{svm_mn_auc}')
#print(results)

SVM= SVC()
svm_grid_values = {'C':[0.001,0.002,0.005,0.03,0.04], 'gamma':[1.5,2,2.5,3,4], 'kernel': ['rbf', 'poly', 'sigmoid']}
scoring=['accuracy','precision','recall','roc_auc']
svm_gridsearch = GridSearchCV(SVM, param_grid = svm_grid_values,scoring = 'accuracy')
svm_best_params = svm_gridsearch.fit(X_train, y_train.ravel())
best_C = svm_best_params.best_estimator_.get_params()['C']
best_gamma = svm_best_params.best_estimator_.get_params()['gamma']
best_kernel = svm_best_params.best_estimator_.get_params()['kernel']

svm_tuned =SVC(C= best_C, gamma = best_gamma , kernel = best_kernel)
svm_tuned.fit(X_train,y_train.ravel())
svm_scores = cross_validate(svm_tuned, X_train, y_train.ravel(), cv=5, scoring=('accuracy','precision','recall','roc_auc'), return_train_score=True)
svm_mn_acc = np.mean(list(svm_scores.values())[2])
#svm_mn_f1 = np.mean(list(svm_scores.values())[3])
svm_mn_precis = np.mean(list(svm_scores.values())[4])
svm_mn_recall = np.mean(list(svm_scores.values())[5])
svm_mn_auc = np.mean(list(svm_scores.values())[6])
global_mean=np.mean([np.array(svm_mn_acc),np.array(svm_mn_precis),np.array(svm_mn_recall),np.array(svm_mn_auc)])

results

svm_tuned =SVC(C= 0.005, gamma=1,kernel = 'poly')
svm_tuned.fit(X_train,y_train.ravel())
y_hat=svm_tuned.predict(X_test)

svm_tuned=pickle.load(open('/content/radial_svm_model (3).sav',mode='rb'))
svm_tuned.fit(X_train,y_train.ravel())
y_hat=svm_tuned.predict(X_test)

print('Radial SVM :')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

print('Radial SVM :')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
radial_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(radial_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="Blues")
plt.title(f'SVM Radial Kernel \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

from sklearn.metrics import confusion_matrix
radial_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(radial_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="Blues")
plt.title(f'SVM Radial Kernel \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

filename = 'radial_svm_model.sav'
pickle.dump(svm_tuned, open(filename, 'wb'))

C=np.arange(0.004,0.01,0.00001)

SVM= LinearSVC()
svm_grid_values = {'C': C}
scoring=['accuracy','precision','recall','roc_auc']
results=[]
for scoring in scoring:
  svm_gridsearch = GridSearchCV(SVM, param_grid = svm_grid_values,scoring = scoring)
  svm_best_params = svm_gridsearch.fit(X_train, y_train.ravel())
  best_C = svm_best_params.best_estimator_.get_params()['C']
  svm_linear=LinearSVC(C= best_C)
  svm_linear.fit(X_train,y_train.ravel())
  svm_scores = cross_validate(svm_linear, X_train, y_train.ravel(), cv=5, scoring=('accuracy','precision','recall','roc_auc'), return_train_score=True)
  svm_mn_acc = np.mean(list(svm_scores.values())[2])
  #svm_mn_f1 = np.mean(list(svm_scores.values())[3])
  svm_mn_precis = np.mean(list(svm_scores.values())[4])
  svm_mn_recall = np.mean(list(svm_scores.values())[5])
  svm_mn_auc = np.mean(list(svm_scores.values())[6])
  global_mean=np.mean([np.array(svm_mn_acc),np.array(svm_mn_precis),np.array(svm_mn_recall),np.array(svm_mn_auc)])
  results.append([scoring,global_mean,best_C])

results

svm_linear=LinearSVC(C= 0.00934)
svm_linear.fit(X_train,y_train.ravel())

svm_linear=pickle.load(open('/content/linear_svm_model (4).sav',mode='rb'))
svm_linear.fit(X_train,y_train.ravel())
y_hat=svm_linear.predict(X_test)

svm_scores = cross_validate(svm_linear, X_train, y_train.ravel(), cv=5, scoring=('accuracy',
'f1','precision','recall','roc_auc'), return_train_score=True)
svm_mn_acc = np.mean(list(svm_scores.values())[2])
svm_mn_f1 = np.mean(list(svm_scores.values())[3])
svm_mn_precis = np.mean(list(svm_scores.values())[4])
svm_mn_recall = np.mean(list(svm_scores.values())[5])
svm_mn_auc = np.mean(list(svm_scores.values())[6])

print(f'accuracy:{svm_mn_acc} f1:{svm_mn_f1} precision:{svm_mn_precis} recall:{svm_mn_recall} auc:{svm_mn_auc}')

y_hat=svm_linear.predict(X_test)

print('linear SVM :')
 print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="Greens")
plt.title(f'SVM Linear Kernel \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

after_train2 = f1_score(y_test,y_hat,average='weighted')
print('F1 after Training' , after_train2.item())

print(accuracy_score(y_test,y_hat,normalize=True))

filename = 'linear_svm_model.sav'
pickle.dump(svm_linear, open(filename, 'wb'))

# open a file, where you stored the pickled data
file = open(filename, 'rb')
# dump information to that file
model = pickle.load(file)
# close the file

"""# Logistic Regression

We iterate though the different scoring method and parameters to understand which is giving us the best results.
"""

logreg = LogisticRegression()
C=np.arange(0.03,2,0.01)
logreg_grid_values = {'penalty': ['l2'],'C':C}
scoring=['f1','accuracy','precision','recall','roc_auc']
results=[]
for score in scoring:
  logreg_gridsearch = GridSearchCV(logreg, param_grid = logreg_grid_values, scoring = score)
  logreg_best_params = logreg_gridsearch.fit(X_train, y_train.ravel())
  best_penalty = logreg_best_params.best_estimator_.get_params()['penalty']
  best_c = logreg_best_params.best_estimator_.get_params()['C']
  logreg_tuned = LogisticRegression(penalty=best_penalty, C= best_c)
  logreg_tuned.fit(X_train,y_train.ravel())
  print(best_c)
  logreg_scores = cross_validate(logreg_tuned, X_train, y_train.ravel(), cv=5, scoring=('accuracy','f1','precision','recall','roc_auc'), return_train_score=True)
  logreg_mn_acc = np.mean(list(logreg_scores.values())[2])
  logreg_mn_f1 = np.mean(list(logreg_scores.values())[3])
  logreg_mn_precis = np.mean(list(logreg_scores.values())[4])
  logreg_mn_recall = np.mean(list(logreg_scores.values())[5])
  logreg_mn_auc = np.mean(list(logreg_scores.values())[6])
  print(f'for {score} accuracy:{logreg_mn_acc} f1:{logreg_mn_f1} precision:{logreg_mn_precis} recall:{logreg_mn_recall} auc:{logreg_mn_auc}')
  global_mean=np.mean([np.array(logreg_mn_acc),np.array(logreg_mn_f1),np.array(logreg_mn_precis),np.array(logreg_mn_auc),np.array(logreg_mn_recall)])
  results.append([score,global_mean,best_c])

results

logreg_tuned=pickle.load(open('/content/logistic_regression_model (2).sav', 'rb'))
logreg_tuned.fit(X_train,y_train.ravel())
y_hat=logreg_tuned.predict(X_test)

logreg_tuned = LogisticRegression(penalty='l2', C= 0.85)
logreg_tuned.fit(X_train,y_train.ravel())
y_hat=logreg_tuned.predict(X_test)

print('Logistic Regression:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="Oranges")
plt.title(f'Logistic Regression \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

filename = 'logistic_regression_model.sav'
pickle.dump(logreg_tuned, open(filename, 'wb'))

"""# Decision Tree"""

max_depth=np.arange(1,800,1)
min_samples_leaf=np.arange(1,800,1)
min_samples_split=np.arange(2,800,1).astype('int64')

dt = DecisionTreeClassifier()
results=[]
for i,a in zip(min_samples_leaf,min_samples_split):
  dt_grid_values = {'max_depth': [max_depth, None],  'min_samples_leaf':[i], 'criterion': ['gini', 'entropy'], 'splitter':['best', 'random'], 'min_samples_split':[a]}
  dt_gridsearch = GridSearchCV(dt, dt_grid_values, refit=True, verbose=2, scoring = 'f1_weighted')
  dt_best_params=dt_gridsearch.fit(X_train,y_train)
  best_max_depth = dt_best_params.best_estimator_.get_params()['max_depth']
  best_min_samples_leaf= dt_best_params.best_estimator_.get_params()['min_samples_leaf']
  best_criterion= dt_best_params.best_estimator_.get_params()['criterion']
  best_splitter= dt_best_params.best_estimator_.get_params()['splitter']
  dt_best_params=DecisionTreeClassifier(max_depth=best_max_depth,min_samples_leaf=best_min_samples_leaf, criterion=best_criterion)
  dt_best_params.fit(X_train,y_train)
  # fit model and get cross validation scores
  dt_scores = cross_validate(dt_best_params, X_train, y_train, cv=5, scoring=('balanced_accuracy', 'f1_weighted','precision','recall','roc_auc'),
  return_train_score=False)
  dt_mn_acc = np.mean(list(dt_scores.values())[2])
  dt_mn_f1 = np.mean(list(dt_scores.values())[3])
  dt_mn_precis = np.mean(list(dt_scores.values())[4])
  dt_mn_recall = np.mean(list(dt_scores.values())[5])
  dt_mn_auc = np.mean(list(dt_scores.values())[6])
  results.append(f'min_samples_leaf {i} accuracy:{dt_mn_acc} f1:{dt_mn_f1} precision:{dt_mn_precis} recall:{dt_mn_recall} auc:{dt_mn_auc} best_max_depth:{best_max_depth} best_criterion:{best_criterion} best_splitter:{ best_splitter} min_samples_split:{a}')

max(results)

results

'min_samples_leaf 85 accuracy:0.7137605495681821 f1:0.7133489410680033 precision:0.6761746851418552 recall:0.7241218896501984 auc:0.7712978686582028 best_max_depth:None best_criterion:gini best_splitter:best min_samples_split:86',

dt_best_params=DecisionTreeClassifier(max_depth=None,min_samples_leaf=65, criterion='gini',splitter='best')
dt_best_params.fit(X_train,y_train)
y_hat=dt_best_params.predict(X_test)

dt_best_params=pickle.load(open('/content/decision_tree_model (3).sav', 'rb'))
dt_best_params.fit(X_train,y_train.ravel())
y_hat=dt_best_params.predict(X_test)

dt_scores = cross_validate(dt_best_params, X_train, y_train, cv=8, scoring=('balanced_accuracy', 'f1_weighted','precision','recall','roc_auc'),
  return_train_score=False)
dt_mn_acc = np.mean(list(dt_scores.values())[2])
dt_mn_f1 = np.mean(list(dt_scores.values())[3])
dt_mn_precis = np.mean(list(dt_scores.values())[4])
dt_mn_recall = np.mean(list(dt_scores.values())[5])

dt_scores

print('Decision Tree:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="Purples")
plt.title(f'Decision Tree \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

filename = 'decision_tree_model.sav'
pickle.dump(dt_best_params, open(filename, 'wb'))

"""# Gradient Boosting

"""

learning_rate=np.arange(0.01,0.1,0.01)
n_estimators=np.arange(100,150,1)

gb = GradientBoostingClassifier()
gb_grid_values ={'learning_rate':[0.005,0.01,0.015,0.02,0.03,0.04,0.05,0.06,0.07,0.08],
'n_estimators':[10,20,30,40,50,60]}
gb_gridsearch = GridSearchCV(gb, gb_grid_values, cv=4, n_jobs=-1, scoring = 'accuracy')
gb_best_params = gb_gridsearch.fit(X_train, y_train.ravel())
best_n_estimators = gb_best_params.best_params_['n_estimators']
best_learning_rate= gb_best_params.best_params_['learning_rate']
gb_tuned = GradientBoostingClassifier(
n_estimators= best_n_estimators,
learning_rate= best_learning_rate)

best_n_estimators

best_learning_rate

gb_tuned=pickle.load(open('/content/gradient_boosting_model (7).sav', 'rb'))

gb_tuned = GradientBoostingClassifier(
n_estimators= 50,
learning_rate= 0.0095)

gb_tuned.fit(X_train,y_train.ravel())
y_hat=gb_tuned.predict(X_test)

print('Gradient Boosting:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

print('Gradient Boosting:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="Oranges")
plt.title(f'Gradient Boosting \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

filename = 'gradient_boosting_model.sav'
pickle.dump(gb_tuned, open(filename, 'wb'))

"""# K-Nearest Neighbour """

knn = KNeighborsClassifier()

leaf_size = list(range(1,10,2))
n_neighbors = list(range(1,10,2))
p=[0.5,1,2,]
weights=['uniform','distance']
algorithm=['ball_tree', 'kd_tree', 'brute','auto']
knn_grid_values = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p,weights=weights,algorithm=algorithm)

knn_gridsearch = GridSearchCV(knn, knn_grid_values, cv=5,scoring = 'accuracy',n_jobs=-1)
knn_best_params = knn_gridsearch.fit(X_train,y_train)

best_leaf_size = knn_best_params.best_estimator_.get_params()['leaf_size']
best_p = knn_best_params.best_estimator_.get_params()['p']
best_NN = knn_best_params.best_estimator_.get_params()['n_neighbors']
best_weights = knn_best_params.best_estimator_.get_params()['weights']
best_algorithm= knn_best_params.best_estimator_.get_params()['algorithm']

print(best_leaf_size,best_p,best_NN,best_weights,best_algorithm)

# tuned model
knn_tuned = KNeighborsClassifier(n_neighbors = best_NN, metric = 'minkowski', p = 2,
leaf_size=best_leaf_size, weights=best_weights,algorithm=best_algorithm)
# fit model and get cross validation scores
knn_scores1 = cross_validate(knn_tuned, X_train, y_train, cv=5, scoring=('accuracy',
'f1','precision','recall','roc_auc'), return_train_score=False)
knn_mn_acc = np.mean(list(knn_scores1.values())[2])
knn_mn_f1 = np.mean(list(knn_scores1.values())[3])
knn_mn_precis = np.mean(list(knn_scores1.values())[4])
knn_mn_recall = np.mean(list(knn_scores1.values())[5])
knn_mn_auc = np.mean(list(knn_scores1.values())[6])

knn_scores1

print(f'accuracy:{knn_mn_acc} f1:{knn_mn_f1} precision:{knn_mn_precis} recall:{knn_mn_recall} auc:{knn_mn_auc}')

knn_tuned=pickle.load(open('/content/knn_model (2).sav', 'rb'))
knn_tuned.fit(X_train,y_train.ravel())
y_hat=knn_tuned.predict(X_test)

knn_tuned.fit(X_train,y_train.ravel())
y_hat=knn_tuned.predict(X_test)

print('KNN Classifier:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="Pastel1")
plt.title(f'KNN Classifier \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

filename = 'knn_model.sav'
pickle.dump(knn_tuned, open(filename, 'wb'))

"""# Random Forest"""

rf = RandomForestClassifier()
# Create hyperparameter grid
rf_grid_values ={
"n_estimators":[5,10,25,50,70,100],
"max_features":[1,2,3,4,5,6],
"min_samples_leaf":[20,21,23,25,30],
"min_samples_split":[2,5,10,15,17,20],
"criterion" : ["gini", "entropy"]}

rf_gridsearch = GridSearchCV(rf, rf_grid_values, cv=4, n_jobs=-1, scoring = 'accuracy')
rf_best_params = rf_gridsearch.fit(X_train, y_train)
best_n_estimators = rf_best_params.best_params_['n_estimators']
best_max_feat = rf_best_params.best_params_['max_features']
best_min_samples_leaf = rf_best_params.best_params_['min_samples_leaf']
best_minsamples_split = rf_best_params.best_params_['min_samples_split']
best_crit = rf_best_params.best_params_['criterion']

best_n_estimators

best_max_feat

best_min_samples_leaf

rf_tuned=RandomForestClassifier(
n_estimators= best_n_estimators,
max_features=best_max_feat,
min_samples_leaf= 10,
min_samples_split= best_minsamples_split,
criterion= best_crit,
oob_score = True)

rf_scores = cross_validate(rf_tuned, X_train, y_train, cv=5, scoring=('accuracy', 'f1','precision','recall','roc_auc'))
rf_mn_acc = np.mean(list(rf_scores.values())[2])
rf_mn_f1 = np.mean(list(rf_scores.values())[3])
rf_mn_precis = np.mean(list(rf_scores.values())[4])
rf_mn_recall = np.mean(list(rf_scores.values())[5])
rf_mn_auc = np.mean(list(rf_scores.values())[6])

rf_scores

rf_tuned.fit(X_train,y_train)
y_hat=rf_tuned.predict(X_test)

rf_tuned=pickle.load(open('/content/random_forest_model (1).sav', 'rb'))
rf_tuned.fit(X_train,y_train.ravel())
y_hat=rf_tuned.predict(X_test)

print('Random Forest Classifier:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="Oranges_r")
plt.title(f'Random Forest \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

filename = 'random_forest_model.sav'
pickle.dump(rf_tuned, open(filename, 'wb'))

"""# Naive Bayes"""

nb = GaussianNB()
nb_scores = cross_validate(nb, X_train, y_train, cv=5, scoring=('accuracy', 'f1','precision','recall','roc_auc'),
return_train_score=False)
nb_mn_acc = np.mean(list(nb_scores.values())[2])
nb_mn_f1 = np.mean(list(nb_scores.values())[3])
nb_mn_precis = np.mean(list(nb_scores.values())[4])
nb_mn_recall = np.mean(list(nb_scores.values())[5])
nb_mn_auc = np.mean(list(nb_scores.values())[6])

nb=pickle.load(open('/content/naive_bayes_model (2).sav', 'rb'))
nb.fit(X_train,y_train.ravel())
y_hat=nb.predict(X_test)

nb.fit(X_train,y_train)
y_hat=nb.predict(X_test)

print('Naive Bayes Classifier:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="BuPu")
plt.title(f'Naive Bayes \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

filename = 'naive_bayes_model.sav'
pickle.dump(nb, open(filename, 'wb'))

"""# Xgboost"""



X_train, X_test, y_train, y_test = train_test_split( X.values, y.values, test_size=0.3, shuffle=False)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

max_depth=np.arange(1,20,1)
min_child_weight=np.arange(1,20,1)

xgb=XGBClassifier()
xgb_grid_values = {'max_depth':max_depth, 'min_child_weight':min_child_weight}
xgb_gridsearch = GridSearchCV(xgb, xgb_grid_values,  cv=4,n_jobs=-1, scoring = 'accuracy')
xgb_best_params = xgb_gridsearch.fit(X_train, y_train.ravel())
best_max_depth = xgb_best_params.best_params_['max_depth']
best_min_child_weight = xgb_best_params.best_params_['min_child_weight']
xgb_tuned= XGBClassifier (max_depth = best_max_depth, min_child_weight= best_min_child_weight)

best_max_depth = xgb_best_params.best_params_['max_depth']
best_min_child_weight = xgb_best_params.best_params_['min_child_weight']

best_min_child_weight

X_train_backup.iloc[:,2:]

xgb_tuned.fit(X_train,y_train.ravel())
importance = xgb_tuned.feature_importances_

importance

X_train=np.take(X_train, features_to_select,axis=1)
X_train=X_train.reshape(X_train.shape[0],X_train.shape[2])
X_test=np.take(X_test, features_to_select,axis=1)
X_test=X_test.reshape(X_test.shape[0],X_test.shape[2])

xgb_scores = cross_validate(xgb_tuned, X_train, y_train.ravel(), cv=10, scoring=('accuracy', 'f1','precision','recall','roc_auc'),
return_train_score=False)
xgb_mn_acc = np.mean(list(xgb_scores.values())[2])
xgb_mn_f1 = np.mean(list(xgb_scores.values())[3])
xgb_mn_precis = np.mean(list(xgb_scores.values())[4])
xgb_mn_recall = np.mean(list(xgb_scores.values())[5])
xgb_mn_auc = np.mean(list(xgb_scores.values())[6])

xgb_mn_auc = np.mean(list(xgb_scores.values())[6])

xgb_mn_auc

xgb_scores

filename = '/content/Xboost_model (1).sav'
xgb_tuned=pickle.load(open(filename, 'rb'))

xgb_tuned.fit(X_train,y_train.ravel())
y_hat=xgb_tuned.predict(X_test)

print('Xgboost Classifier:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="gray")
plt.title(f'Xgboost \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

filename = 'Xboost_model.sav'
pickle.dump(xgb_tuned, open(filename, 'wb'))

"""# Catboost"""

X=main_data_red.iloc[:,1:]
y=main_data_red.iloc[:,:1]
#X.drop(X.columns[index_to_drop],axis=1,inplace=True)

X_train, X_test, y_train, y_test = train_test_split( X, y.values, test_size=0.3, shuffle=False)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

#scaler=MinMaxScaler()
#scaler=QuantileTransformer()
scaler=RobustScaler()

X_train_norm=scaler.fit_transform(X_train)
X_test_norm=scaler.transform(X_test)

X_train=pd.concat([X_train.iloc[:,:7],X_train.iloc[:,7:].astype('int64')],axis=1)  
X_train.columns=X.columns
X_test=pd.concat([X_test.iloc[:,:7],X_test.iloc[:,7:].astype('int64')],axis=1)  
X_test.columns=X.columns

scaler=RobustScaler()
X_train_norm=scaler.fit_transform(X_train.iloc[:,0:6])
X_test_norm=scaler.transform(X_test.iloc[:,0:6])
X_train=np.concatenate([X_train_norm,X_train.iloc[:,6:].astype('int64')],axis=1,)
X_test=np.concatenate([X_test_norm,X_test.iloc[:,6:].astype('int64')],axis=1)

#categorical_features_indices = np.where(X_train.dtypes != np.float64)[0]
categorical_features_indices=np.arange(6,24,1)

model = CatBoostClassifier(**params)

parameters

model1 = CatBoostClassifier()
model1.set_params(**params)

model = CatBoostClassifier(
    custom_loss=['Accuracy'],
    random_seed=42,
    logging_level='Silent'
)

model.fit(
    X_train, y_train,
    #cat_features=categorical_features_indices,
    eval_set=(X_test, y_test),
    #logging_level='Verbose',  
    plot=True
);

y_hat = model.predict(X_test)

params = {
    'iterations': 1000,
    'learning_rate': 0.1,
    'eval_metric': 'Accuracy',
    'random_seed': 42,
    'logging_level': 'Silent',
    'use_best_model': False
}
train_pool = Pool(X_train, y_train)#,cat_features=categorical_features_indices) 
validate_pool = Pool(X_test, y_test)#,cat_features=categorical_features_indices)

model = CatBoostClassifier(**params)

model = CatBoostClassifier(**params)
model.fit(train_pool, eval_set=validate_pool)

best_model_params = params.copy()
best_model_params.update({
    'use_best_model': True
})
best_model = CatBoostClassifier(**best_model_params)
best_model.fit(train_pool, eval_set=validate_pool);

print('Simple model validation accuracy: {:.4}'.format(
    accuracy_score(y_test, model.predict(X_test))
))
print('')

print('Best model validation accuracy: {:.4}'.format(
    accuracy_score(y_test, best_model.predict(X_test))
))

shap_values = model.get_feature_importance(Pool(X_train, y_train), type='ShapValues')

a= CatBoostClassifier(**best_model_params)

a.get

shap_values=shap_values[:,:-1]

shap.summary_plot(shap_values, X_train)

index_to_drop=np.where(feature_importances<1)

filename = '/content/Catboost_model (3).sav'
model=pickle.load(open(filename, 'rb'))

filename = '/content/Catboost_model.sav'
pickle.dump(model,open(filename, 'wb'))

model.save_model("model")

from_file = CatBoostClassifier()

shap_values = model.get_feature_importance(Pool(X_train, y_train))

a=from_file.load_model("model")

params=a.get_all_params()

y_hat=best_model.predict(X_test)

model.fit(train_pool, eval_set=validate_pool)
y_hat=best_model.predict(X_test)

y_hat=model.predict(X_train)

print('Catboost Classifier:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="crest")
plt.title(f'Catboost \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

model = CatBoostClassifier(iterations=50, random_seed=42, logging_level='Silent').fit(train_pool)
feature_importances = model.get_feature_importance(train_pool)
feature_names = X.columns
for score, name in sorted(zip(feature_importances, feature_names), reverse=True):
    print('{}: {}'.format(name, score))

"""# Ensembling

"""

clf1 = KNeighborsClassifier(n_neighbors = best_NN, metric = 'minkowski', p = best_p,
leaf_size=best_leaf_size, weights=best_weights,algorithm=best_algorithm)
clf2 = RandomForestClassifier(
n_estimators= best_n_estimators,
max_features=best_max_feat,
min_samples_leaf= best_min_samples_leaf,
min_samples_split= best_minsamples_split,
criterion= best_crit,
oob_score = True)
clf3 = GaussianNB()
clf4= GradientBoostingClassifier(
n_estimators= best_n_estimators,
learning_rate= best_learning_rate)
clf5=SVC(C= best_C, gamma = best_gamma , kernel = best_kernel)
clf6=LinearSVC(C= best_C)
lr = LogisticRegression(penalty=best_penalty, C= best_c)
xgb=XGBClassifier (max_depth = best_max_depth, min_child_weight= best_min_child_weight)
ensemble = VotingClassifier(estimators=[('KNN',clf1),('Random Forest',clf2),('Gaussian',clf3),('Grad Boost',clf4),('XGB',xgb),('SVM',clf5),('LinearSVM',clf6),('LogReg',lr)])

ensemble = VotingClassifier(estimators=[('KNN',knn_tuned),('Random Forest',rf_tuned),('Gaussian',nb),('Grad Boost',gb_tuned),('XGB',xgb_tuned),('SVM',svm_tuned),('LinearSVM',svm_linear),('LogReg',logreg_tuned)])

print(best_max_depth)

ensemble.fit(X_train,y_train.ravel())
y_hat=ensemble.predict(X_test)

print('Ensemble Classifier:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="gray")
plt.title(f'Ensembling \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

filename = 'Ensemble_model.sav'
pickle.dump(ensemble, open(filename, 'wb'))

"""# Artificial Neural Network

### Keras





"""

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)

model = Sequential()
model.add(Dense(16, input_dim=22, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.45))	
model.add(Dense(16, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.45))
model.add(Dense(1, activation='sigmoid'))

for i in range(len(temp_weights)):
    model.layers[i].set_weights(temp_weights[i])

opt = tf.keras.optimizers.Adagrad(learning_rate=0.0001)

model.compile(loss='binary_crossentropy', optimizer=opt , metrics='accuracy')

model=model.save('/content/saved_model1')

ANN=keras.models.load_model('/content/saved_model/')

for i in range(1,1000):
  #opt = keras.optimizers.Adam(learning_rate=0.0001)
  opt = tf.keras.optimizers.Adagrad(learning_rate=0.0001)
  #opt = tf.keras.optimizers.RMSprop(learning_rate=0.001,momentum=0.9)
  #opt = tf.keras.optimizers.Adamax(learning_rate=0.001)
  #opt = tf.keras.optimizers.Nadam(learning_rate=0.00001,)
  #opt = keras.optimizers.SGD(learning_rate=0.001,momentum=0.9)
  
  model=keras.models.load_model('/content/saved_model/')
  model.compile(loss='binary_crossentropy', optimizer=opt , metrics='accuracy')
  history=model.fit(X_train, y_train, epochs=1000, batch_size=20,validation_data=(X_test,y_test),shuffle=True,callbacks=[es])
  _, train_acc = model.evaluate(X_train, y_train, verbose=0)
  _, test_acc = model.evaluate(X_test, y_test, verbose=0)
  print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
  model=model.save('/content/saved_model')

# evaluate the model
model=keras.models.load_model('/content/saved_model/')
model.compile(loss='binary_crossentropy', optimizer=opt , metrics='accuracy')
_, train_acc = model.evaluate(X_train, y_train, verbose=0)
_, test_acc = model.evaluate(X_test, y_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

y_hat=model.predict(X_test)
y_hat=np.where(y_hat>0.5,1,0)

print('Ensemble Classifier:')
print (classification_report(y_test, y_hat, target_names=['bad responder', 'good responder']))

from sklearn.metrics import confusion_matrix
y_hat=np.where(y_hat>0.5,1,0)
log_reg_confusion_matrix=confusion_matrix(y_test,y_hat)
cm_df = pd.DataFrame(log_reg_confusion_matrix,
                     index = ['Bad Responder','Good Responder'], 
                     columns = ['Bad Responder','Good Responder'])

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True,fmt="",cmap="binary")
plt.title(f'ANN \nAccuracy Score:{accuracy_score(y_test, y_hat):.3f} \nF1 score:{f1_score(y_test, y_hat):.3f}')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

"""# ROC"""

# PLOT ROC CURVE

plt.figure(figsize=(15,10))
ax=plt.gca()
ax.set_xticks(np.arange(0,1.05,0.05)) 
ax.set_yticks(np.arange(0,1.05,0.05))
ax.set_xlabel('X axis', fontsize = 13)
ax.set_ylabel('Y axis', fontsize = 13)
svm_disp = plot_roc_curve(svm_tuned, X_test, y_test,ax=ax)
gb_disp = plot_roc_curve(gb_tuned, X_test, y_test, ax=svm_disp.ax_)
xgb_disp = plot_roc_curve(xgb_tuned, X_test, y_test, ax=gb_disp.ax_)
cb_disp = plot_roc_curve(best_model, X_test, y_test, ax=xgb_disp.ax_)
knn_disp = plot_roc_curve(knn_tuned, X_test, y_test, ax=cb_disp.ax_)
nb_disp=plot_roc_curve(nb, X_test, y_test, ax=knn_disp.ax_)
lr_disp=plot_roc_curve(logreg_tuned, X_test, y_test, ax=nb_disp.ax_)
linear_svm_disp = plot_roc_curve(svm_linear, X_test, y_test,ax=lr_disp.ax_)
rf_disp = plot_roc_curve(rf_tuned, X_test, y_test,ax=linear_svm_disp.ax_)
dt_disp=plot_roc_curve(dt_best_params, X_test, y_test,ax=rf_disp.ax_)
plt.title('Receiver Operating Characteristic curve',fontsize=15)
plt.show()

"""# Plots

"""

def plot_feature_importance(importance,names,model_type,ax,show_null=False):

#Create arrays from feature importance and feature names
  feature_importance = np.array(importance)
  feature_names = np.array(names)

  #Create a DataFrame using a Dictionary
  data={'feature_names':feature_names,'feature_importance':feature_importance}
  fi_df = pd.DataFrame(data)

  #Sort the DataFrame in order decreasing feature importance
  fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)

  #Define size of bar plot
  plt.figure(figsize=(10,8))
  sns.set_theme(style="whitegrid")
  #Plot Searborn bar chart
  if show_null == True:
    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'],palette="Blues_d",ax=ax)
  else:
    fi_df=fi_df[fi_df['feature_importance']>0.01]
    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'], palette="Blues_d",ax=ax)
  #Add chart labels
  plt.title(model_type + 'FEATURE IMPORTANCE')
  plt.xlabel('IMPORTANCE')
  plt.ylabel('FEATURES')

X.iloc[:,2:].columns

xgb_tuned.feature_importances_

#plot_feature_importance(xgb_tuned.feature_importances_,X.iloc[:,2:].columns,'XGBoost ',ax=None,show_null=True)
plot_feature_importance(gb_tuned.feature_importances_,X.iloc[:,2:].columns,'GBoost ',ax=None,show_null=False)

models[0].summary()

for name,model in models:
  print(model.feature_importances_)

models = []
models.append(('XGB', xgb_tuned)) 
#models.append(('GB', gb_tuned)) 
#models.append(('Catboost', model))
#models.append(('Random Forest', rf_tuned))
#models.append(('Radial SVM',svm_tuned))
#models.append(('Linear SVM',svm_linear))
#models.append(('Decision Tree',dt_best_params))
#models.append(('Logistic Regression',logreg_tuned))
#models.append(('Ensemble',ensemble))

for name,model in models:
  plot_feature_importance(model.feature_importances_,X.iloc[:,2:].columns,name+'',ax=None,show_null=False)
  ax

"""#Repeated K_SPLIT_VALIDATION

"""

best_model_params.update({
    'use_best_model': False
})

X_norm.shape

X_norm= np.concatenate((X_train,X_test),axis=0)
y=np.concatenate((y_train,y_test),axis=0)

# Spot-Check Algorithms
from sklearn.model_selection import cross_val_score
models = []
models.append(('XGB', xgb_tuned)) 
models.append(('GB', gb_tuned)) 
models.append(('Catboost', model))
models.append(('Random Forest', rf_tuned))
models.append(('Radial SVM',svm_tuned))
models.append(('Linear SVM',svm_linear))
models.append(('Decision Tree',dt_best_params))
models.append(('Logistic Regression',logreg_tuned))
models.append(('Ensemble',ensemble))
# evaluate each model in turn
results = []
names = []
for name, model in models:
    kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats = 3, random_state=1)
    cv_results = cross_val_score(model, X_norm, y.ravel(), cv=kfold, scoring='accuracy') 
    results.append(cv_results)
    names.append(name)
    msg = "%s: %.2f (%.3f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

results=sklearn.model_selection.permutation_test_score(xgb_tuned,X_norm,y.ravel(),cv=10,n_jobs=-1)

cv_results

results

from mlxtend.evaluate import paired_ttest_5x2cv
# check if difference between algorithms is real
t, p = paired_ttest_5x2cv(estimator1=xgb_tuned, 
                          estimator2=gb_tuned,
                          estimator3=gb_tuned,  
                          X=X_test, 
                          y=y_test.ravel(), 
                          scoring='accuracy', 
                          random_seed=1)
# summarize
print(f'The P-value is = {p:.3f}')
print(f'The t-statistics is = {t:.3f}')
# interpret the result
if p <= 0.05:
    print('Since p<0.05, We can reject the null-hypothesis that both models perform equally well on this dataset. We may conclude that the two algorithms are significantly different.')
else:
    print('Since p>0.05, we cannot reject the null hypothesis and may conclude that the performance of the two algorithms is not significantly different.')

"""#STATISTICAL TEST

H0= Model has a true rate of prediction as 0.5 (pure chance)
H1= Model has a true rate of prediction greater than 0.5
"""

alpha=0.01 #set a confidence a value

xgb_tuned.fit(X_train,y_train.ravel())
y_hat=xgb_tuned.predict(X_test)

corrected=np.count_nonzero(y_test.ravel()==y_hat) #count number of patients correctly identified

corrected

from scipy.stats import binom
prob = 1 - binom.cdf(corrected, 1096, 0.5) #
print(format(prob*100,'.20f')+' %')

"""
The probability to classifiy 821 patients out of 1096 correctly by chance is lower than 1%. We can reject the null hypothesis and conclude that the model has a true prediction rate greater than 0.5, with a confidence level of 99 %."""